{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - BD Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext # for RDDs\n",
    "from pyspark.sql import SparkSession # for DFs\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "                    .appName('BDM_project3')\n",
    "                    .getOrCreate()\n",
    "        ) # for DFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- authors: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- n_citation: long (nullable = true)\n",
      " |-- references: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- venue: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading in all the files\n",
    "files = [\"dblp-ref-0.json\", \"dblp-ref-1.json\", \"dblp-ref-2.json\", \"dblp-ref-3.json\"]\n",
    "\n",
    "papers_df = (spark.read\n",
    "             .option(\"inferSchema\", True) # Letting Spark itself define the schema\n",
    "             .json(files) \n",
    "            )\n",
    "\n",
    "papers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in only one file (for testing purposes\n",
    "'''\n",
    "papers_df = (spark.read\n",
    "             .option(\"inferSchema\", True) # Letting Spark itself define the schema\n",
    "             .json(\"dblp-ref-3.json\")\n",
    "            )\n",
    "\n",
    "papers_df.printSchema()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|            abstract|             authors|                  id|n_citation|          references|               title|               venue|year|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "|The purpose of th...|[Makoto Satoh, Ry...|00127ee2-cb05-48c...|         0|[51c7e02e-f5ed-43...|Preliminary Desig...|international con...|2013|\n",
      "|This paper descri...|[Gareth Beale, Gr...|001c58d3-26ad-46b...|        50|[10482dd3-4642-41...|A methodology for...|visual analytics ...|2011|\n",
      "|This article appl...|[Altaf Hossain, F...|001c8744-73c4-4b0...|        50|[2d84c0f2-e656-4c...|Comparison of GAR...|pattern recogniti...|2009|\n",
      "|                NULL|[Jea-Bum Park, By...|00338203-9eb3-40c...|         0|[8c78e4b0-632b-42...|Development of Re...|                    |2011|\n",
      "|                NULL|[Giovanna Guerrin...|0040b022-1472-4f7...|         2|                NULL|Reasonig about Se...|                    |1998|\n",
      "+--------------------+--------------------+--------------------+----------+--------------------+--------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subset of the dataframe\n",
    "papers_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3079007, 8)\n"
     ]
    }
   ],
   "source": [
    "# Shape of the dataframe\n",
    "print((papers_df.count(), len(papers_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " abstract   | The purpose of this study is to develop a learning tool for high school students studying the scientific aspects of information and communication net- works. More specifically, we focus on the basic principles of network proto- cols as the aim to develop our learning tool. Our tool gives students hands-on experience to help understand the basic principles of network protocols. \n",
      " authors    | [Makoto Satoh, Ryo Muramatsu, Mizue Kayama, Kazunori Itoh, Masami Hashimoto, Makoto Otani, Michio Shimizu, Masahiko Sugimoto]                                                                                                                                                                                                                                                               \n",
      " id         | 00127ee2-cb05-48ce-bc49-9de556b93346                                                                                                                                                                                                                                                                                                                                                        \n",
      " n_citation | 0                                                                                                                                                                                                                                                                                                                                                                                           \n",
      " references | [51c7e02e-f5ed-431a-8cf5-f761f266d4be, 69b625b9-ebc5-4b60-b385-8a07945f5de9]                                                                                                                                                                                                                                                                                                                \n",
      " title      | Preliminary Design of a Network Protocol Learning Tool Based on the Comprehension of High School Students: Design by an Empirical Study Using a Simple Mind Map                                                                                                                                                                                                                             \n",
      " venue      | international conference on human-computer interaction                                                                                                                                                                                                                                                                                                                                      \n",
      " year       | 2013                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# First row\n",
    "papers_df.show(n=1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abstract': 530475,\n",
       " 'authors': 4,\n",
       " 'id': 0,\n",
       " 'n_citation': 0,\n",
       " 'references': 362865,\n",
       " 'title': 0,\n",
       " 'venue': 0,\n",
       " 'year': 0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number on NaN values in each column \n",
    "# https://stackoverflow.com/questions/44627386/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe\n",
    "\n",
    "Dict_Null = {col:papers_df.filter(papers_df[col].isNull()).count() for col in papers_df.columns}\n",
    "Dict_Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>n_citation</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>3079007</td>\n",
       "      <td>3079007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>35.220902713114974</td>\n",
       "      <td>2007.7665994263734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>157.70065110545153</td>\n",
       "      <td>7.8165384986224415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>0</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>73362</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary          n_citation                year\n",
       "0   count             3079007             3079007\n",
       "1    mean  35.220902713114974  2007.7665994263734\n",
       "2  stddev  157.70065110545153  7.8165384986224415\n",
       "3     min                   0                1936\n",
       "4     max               73362                2018"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics (for numeric columns)\n",
    "papers_df.describe(\"n_citation\", \"year\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keeping the English documents\n",
    "\n",
    "# !pip install langdetect\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from langdetect import detect, LangDetectException\n",
    "# Only keeping the English documents\n",
    "def detect_language(text):\n",
    "    if text:\n",
    "        try:\n",
    "            return detect(text)\n",
    "        except:\n",
    "            return 'unknown'\n",
    "    return 'unknown'\n",
    "detect_language_udf = udf(detect_language, StringType())\n",
    "papers_df = papers_df.withColumn(\"language\", detect_language_udf(papers_df.abstract))\n",
    "papers_df = papers_df.filter(papers_df.language == 'en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords (with Gensim)\n",
    "# !pip install gensim\n",
    "from gensim.parsing.preprocessing import remove_stopwords #!pip install gensim\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    if text is not None:\n",
    "        return remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove custom stopwords\n",
    "custom_stop_words = [ 'doi',\n",
    "'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure','rights',\n",
    "'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier',\n",
    "'PMC', 'CZI', 'www']\n",
    "def remove_custom_stop_words(text):\n",
    "    if text is not None:\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word not in custom_stop_words]\n",
    "        return ' '.join(filtered_words)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "import re\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    if text is not None:\n",
    "        return re.sub(r'[!()\\[\\]{};:\\'\"\\,<>./?@#$%^&*_~]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import lower\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "\n",
    "# Create a user-defined function (UDF)\n",
    "remove_stop_words_udf = F.udf(remove_stop_words, StringType()) # Default return type is string\n",
    "custom_stop_words_udf = F.udf(remove_custom_stop_words, StringType()) # Default return type is string\n",
    "remove_punctuation_udf = F.udf(remove_punctuation, StringType()) # Default return type is string\n",
    "\n",
    "# Apply the UDF \n",
    "# Remove stop words\n",
    "papers_df = papers_df.withColumn(\"abstract\", remove_stop_words_udf(papers_df[\"abstract\"]))\n",
    "papers_df = papers_df.withColumn(\"title\", remove_stop_words_udf(papers_df[\"title\"]))\n",
    "\n",
    "# Convert into a lowercase\n",
    "papers_df = papers_df.withColumn('abstract', lower(papers_df['abstract']))\n",
    "papers_df = papers_df.withColumn('title', lower(papers_df['title']))\n",
    "\n",
    "# Remove custom stop words\n",
    "papers_df = papers_df.withColumn(\"abstract\", custom_stop_words_udf(papers_df[\"abstract\"]))\n",
    "papers_df = papers_df.withColumn(\"title\", custom_stop_words_udf(papers_df[\"title\"]))\n",
    "\n",
    "# Remove punctuation\n",
    "papers_df = papers_df.withColumn('abstract', remove_punctuation_udf(papers_df['abstract']))\n",
    "papers_df = papers_df.withColumn('title', remove_punctuation_udf(papers_df['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " abstract   | the purpose study develop learning tool high school students studying scientific aspects information communication net- works more specifically focus basic principles network proto- cols aim develop learning tool our tool gives students hands-on experience help understand basic principles network protocols \n",
      " authors    | [Makoto Satoh, Ryo Muramatsu, Mizue Kayama, Kazunori Itoh, Masami Hashimoto, Makoto Otani, Michio Shimizu, Masahiko Sugimoto]                                                                                                                                                                                       \n",
      " id         | 00127ee2-cb05-48ce-bc49-9de556b93346                                                                                                                                                                                                                                                                                \n",
      " n_citation | 0                                                                                                                                                                                                                                                                                                                   \n",
      " references | [51c7e02e-f5ed-431a-8cf5-f761f266d4be, 69b625b9-ebc5-4b60-b385-8a07945f5de9]                                                                                                                                                                                                                                        \n",
      " title      | preliminary design network protocol learning tool based comprehension high school students design empirical study simple mind map                                                                                                                                                                                   \n",
      " venue      | international conference on human-computer interaction                                                                                                                                                                                                                                                              \n",
      " year       | 2013                                                                                                                                                                                                                                                                                                                \n",
      " language   | en                                                                                                                                                                                                                                                                                                                  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "papers_df.show(n=1, truncate=False, vertical=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, Word2Vec\n",
    "\n",
    "# Converting data for ML algorithms\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol=\"abstract\", outputCol=\"abstract_words\")\n",
    "papers_df = tokenizer.transform(papers_df)\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"title_words\")\n",
    "papers_df = tokenizer.transform(papers_df)\n",
    "\n",
    "# Train Word2Vec model on abstract\n",
    "word2Vec_abstract = Word2Vec(vectorSize=3, minCount=0, inputCol=\"abstract_words\", outputCol=\"abstract_word_vectors\")\n",
    "model_abstract = word2Vec_abstract.fit(papers_df)\n",
    "papers_df = model_abstract.transform(papers_df)\n",
    "\n",
    "# Train Word2Vec model on title\n",
    "word2Vec_title = Word2Vec(vectorSize=3, minCount=0, inputCol=\"title_words\", outputCol=\"title_word_vectors\")\n",
    "model_title = word2Vec_title.fit(papers_df)\n",
    "papers_df = model_title.transform(papers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+\n",
      "|title_words                                                                                                                     |title_word_vectors                                              |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+\n",
      "|[a, heterogeneous, system, real-time, detection, adaboost]                                                                      |[0.1047421395778656,-0.19825784070417285,-0.2943623377941549]   |\n",
      "|[a, novel, conformal, jigsaw, ebg, structure, design]                                                                           |[-0.01776855013200215,0.11805542784609964,-0.18724185600876808] |\n",
      "|[a, source-seeking, strategy, autonomous, underwater, vehicle, on-line, field, estimation]                                      |[0.1360605876478884,-0.14277052051491207,-0.3002888990773095]   |\n",
      "|[studying, linguistic, changes, 200, years, newspapers, resilient, words, analysis]                                             |[0.04104703077529039,0.03502231393940747,-0.13957981206476688]  |\n",
      "|[small, secret, exponent, attacks, rsa, unbalanced, prime, factors]                                                             |[0.10223487787880003,-0.09928686544299126,-0.02722227937192656] |\n",
      "|[automatic, container, code, recognition, spatial, transformer, networks, connected, component, region, proposals]              |[-0.04908238766206936,-0.18849172203970904,-0.3219423881647262] |\n",
      "|[effective, solution, medical, tourism, aggregative, data, mining, approach]                                                    |[0.19382472697179765,0.22789747826755047,-0.21814477816224098]  |\n",
      "|[modeling, physical, structure, additional, constraints, stereoscopic, optical, see-through, head-mounted, display, calibration]|[0.10943500951609829,-0.09383618687702851,-0.12766454030166974] |\n",
      "|[delay, analysis, completely, irrepressible, sequences, mobile, ad, hoc, networks]                                              |[0.398432172420952,-0.6697541200038459,-0.12279291916638613]    |\n",
      "|[a, high-performance, portable, abstract, interface, explicit, simd, vectorization]                                             |[-0.059244513977319,0.12488660600502044,-0.08589860587380826]   |\n",
      "|[index, appearance, record, transforming, rabin, automata, parity, automata]                                                    |[0.02916524710599333,-0.016084000671980903,-0.23988191834359895]|\n",
      "|[one-page, multimedia, interactive, map]                                                                                        |[0.10923270578496158,-0.01962689310312271,-0.28053173515945673] |\n",
      "|[topology, experimentation, zigbee, wireless, sensor, network]                                                                  |[0.27419622242450714,-0.8598120932777722,-0.19031405852486688]  |\n",
      "|[tools, , techniques, malware, analysis, classification]                                                                        |[-0.06975540394584337,0.08224367288251717,-0.32505434254805243] |\n",
      "|[heterogeneous, information, network, embedding, meta, path, based, proximity]                                                  |[0.14811114501208067,-0.30205411347560585,-0.2722436796175316]  |\n",
      "|[metaheuristic, design, pattern, visitor, genetic, operators]                                                                   |[0.107433859569331,0.04943125555291772,-0.27108489970366156]    |\n",
      "|[on, computation, centrality, metrics, network, security, mesh, networks]                                                       |[0.14516419870778918,-0.4716096834745258,-0.16402406885754317]  |\n",
      "|[the, difference-of-datasets, framework, a, statistical, method, discover, insight]                                             |[-0.0016796880518086255,0.1508608547737822,-0.2748727796733874] |\n",
      "|[network, calculus, worst-case, latency, analysis, ttethernet, preemption, transmission, mode]                                  |[0.14668354661100438,-0.20877082770069438,-0.07435727962810132] |\n",
      "|[secret, key, agreement, discussion, rate, constraints]                                                                         |[0.13464536269505817,-0.13673448686798412,-0.1096798077536126]  |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the results\n",
    "papers_df.select(\"title_words\", \"title_word_vectors\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>id</th>\n",
       "      <th>n_citation</th>\n",
       "      <th>references</th>\n",
       "      <th>title</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "      <th>language</th>\n",
       "      <th>abstract_words</th>\n",
       "      <th>title_words</th>\n",
       "      <th>abstract_word_vectors</th>\n",
       "      <th>title_word_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adaboost algorithm based haar-like features ac...</td>\n",
       "      <td>[Zheng Xu, Runbin Shi, Zhihao Sun, Yaqi Li, Yu...</td>\n",
       "      <td>001eef4f-1d00-4ae6-8b4f-7e66344bbc6e</td>\n",
       "      <td>0</td>\n",
       "      <td>[0a11984c-ab6e-4b75-9291-e1b700c98d52, 1f4152a...</td>\n",
       "      <td>a heterogeneous system real-time detection ada...</td>\n",
       "      <td>high performance computing and communications</td>\n",
       "      <td>2016</td>\n",
       "      <td>en</td>\n",
       "      <td>[adaboost, algorithm, based, haar-like, featur...</td>\n",
       "      <td>[a, heterogeneous, system, real-time, detectio...</td>\n",
       "      <td>[0.11266263969710294, 0.14881521901033357, 0.0...</td>\n",
       "      <td>[0.1047421395778656, -0.19825784070417285, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in paper kind novel jigsaw ebg structure desig...</td>\n",
       "      <td>[Yufei Liang, Yan Zhang, Tao Dong, Shan-wei Lu]</td>\n",
       "      <td>002e0b7e-d62f-4140-b015-1fe29a9acbaa</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>a novel conformal jigsaw ebg structure design</td>\n",
       "      <td>international conference on conceptual structures</td>\n",
       "      <td>2016</td>\n",
       "      <td>en</td>\n",
       "      <td>[in, paper, kind, novel, jigsaw, ebg, structur...</td>\n",
       "      <td>[a, novel, conformal, jigsaw, ebg, structure, ...</td>\n",
       "      <td>[0.26782341763426204, 0.08140046438692432, 0.0...</td>\n",
       "      <td>[-0.01776855013200215, 0.11805542784609964, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this paper studies problem autonomous underwat...</td>\n",
       "      <td>[Xiaodong Ai, Keyou You, Shiji Song]</td>\n",
       "      <td>00352759-f0a7-4678-82ae-fed68c700da6</td>\n",
       "      <td>0</td>\n",
       "      <td>[1862a08a-08c6-4ab1-a214-8932bbd0d2d9, 7bcea2f...</td>\n",
       "      <td>a source-seeking strategy autonomous underwate...</td>\n",
       "      <td>international conference on control, automatio...</td>\n",
       "      <td>2016</td>\n",
       "      <td>en</td>\n",
       "      <td>[this, paper, studies, problem, autonomous, un...</td>\n",
       "      <td>[a, source-seeking, strategy, autonomous, unde...</td>\n",
       "      <td>[0.15594720700207879, 0.2469724685748053, 0.06...</td>\n",
       "      <td>[0.1360605876478884, -0.14277052051491207, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this paper presents methodology analyze lingui...</td>\n",
       "      <td>[Vincent Buntinx, Cyril Bornet, Frédéric Kaplan]</td>\n",
       "      <td>01522369-3b88-4256-99d4-4e0c1de9f1ba</td>\n",
       "      <td>0</td>\n",
       "      <td>[426b57a8-2e7d-498d-9a57-c09983ae0699, 6499d57...</td>\n",
       "      <td>studying linguistic changes 200 years newspape...</td>\n",
       "      <td>Frontiers in Digital Humanities</td>\n",
       "      <td>2017</td>\n",
       "      <td>en</td>\n",
       "      <td>[this, paper, presents, methodology, analyze, ...</td>\n",
       "      <td>[studying, linguistic, changes, 200, years, ne...</td>\n",
       "      <td>[-0.037486696774049574, 0.17322031537848478, 0...</td>\n",
       "      <td>[0.04104703077529039, 0.03502231393940747, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boneh durfee eurocrypt 1999 proposed polynomia...</td>\n",
       "      <td>[Atsushi Takayasu, Noboru Kunihiro]</td>\n",
       "      <td>01537b60-9ae2-4684-a1fa-e688e7757e6f</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>small secret exponent attacks rsa unbalanced p...</td>\n",
       "      <td>international symposium on information theory ...</td>\n",
       "      <td>2016</td>\n",
       "      <td>en</td>\n",
       "      <td>[boneh, durfee, eurocrypt, 1999, proposed, pol...</td>\n",
       "      <td>[small, secret, exponent, attacks, rsa, unbala...</td>\n",
       "      <td>[0.18371008007281173, 0.13022235570673457, -0....</td>\n",
       "      <td>[0.10223487787880003, -0.09928686544299126, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44845</th>\n",
       "      <td>we analyse problem aggregating judgments multi...</td>\n",
       "      <td>[Irem Bozbay, Franz Dietrich, Hans Peters]</td>\n",
       "      <td>fab4bc37-adaf-46a8-928f-cb232d14a574</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>judgment aggregation search truth</td>\n",
       "      <td></td>\n",
       "      <td>2014</td>\n",
       "      <td>en</td>\n",
       "      <td>[we, analyse, problem, aggregating, judgments,...</td>\n",
       "      <td>[judgment, aggregation, search, truth]</td>\n",
       "      <td>[0.020626525186220712, 0.05914261931967404, 0....</td>\n",
       "      <td>[0.07361198449507356, 0.067861866671592, -0.19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44846</th>\n",
       "      <td>the utilization icts creating new jobs elimina...</td>\n",
       "      <td>[Marios Pappas, Yannis Papagerasimou, Athanasi...</td>\n",
       "      <td>fb60d6a0-ac9f-40cf-a4b7-9dfbcd0951e7</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>ict-based innovation employability women</td>\n",
       "      <td></td>\n",
       "      <td>2017</td>\n",
       "      <td>en</td>\n",
       "      <td>[the, utilization, icts, creating, new, jobs, ...</td>\n",
       "      <td>[ict-based, innovation, employability, women]</td>\n",
       "      <td>[-0.1855058832864823, -0.0411357720350397, 0.4...</td>\n",
       "      <td>[-0.05307137384079397, 0.08278964925557375, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44847</th>\n",
       "      <td>in infinite horizon inventory sales model sell...</td>\n",
       "      <td>[Anita van den Berg, Jean-Jacques Herings, Han...</td>\n",
       "      <td>fc0809cd-e8b5-4f50-a929-e09312c59fe0</td>\n",
       "      <td>0</td>\n",
       "      <td>[35a8f1b8-4e51-4928-9603-1f6a1872d02b, 460f12f...</td>\n",
       "      <td>the economic order decision continuous dynamic...</td>\n",
       "      <td>Operations Research Letters</td>\n",
       "      <td>2017</td>\n",
       "      <td>en</td>\n",
       "      <td>[in, infinite, horizon, inventory, sales, mode...</td>\n",
       "      <td>[the, economic, order, decision, continuous, d...</td>\n",
       "      <td>[0.3582092604690021, -0.023328542408923948, 0....</td>\n",
       "      <td>[0.19857570487591955, 0.10304184188134968, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44848</th>\n",
       "      <td>infrared imaging technology study deep-space b...</td>\n",
       "      <td>[Michele Dei, Stepan Sutula, Jose Cisneros, Er...</td>\n",
       "      <td>fc27af72-9f8c-4bda-8f19-11100096ae59</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>a robust 966-db-sndr 50-khz-bandwidth switched...</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>2017</td>\n",
       "      <td>en</td>\n",
       "      <td>[infrared, imaging, technology, study, deep-sp...</td>\n",
       "      <td>[a, robust, 966-db-sndr, 50-khz-bandwidth, swi...</td>\n",
       "      <td>[0.21958948684480498, -0.06015576256782208, 0....</td>\n",
       "      <td>[-0.004775028083134782, -0.016148306598717518,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44849</th>\n",
       "      <td>taking net power output optimization objective...</td>\n",
       "      <td>[Chao He, You-zhou Jiao, Chaochao Tian, Zhenfe...</td>\n",
       "      <td>fcdeea27-105a-45bb-b34d-0eecf6f415bf</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>the exergy loss distribution heat transfer cap...</td>\n",
       "      <td>Entropy</td>\n",
       "      <td>2017</td>\n",
       "      <td>en</td>\n",
       "      <td>[taking, net, power, output, optimization, obj...</td>\n",
       "      <td>[the, exergy, loss, distribution, heat, transf...</td>\n",
       "      <td>[0.28669231532641304, 0.00015438871072014657, ...</td>\n",
       "      <td>[0.09038282524455678, -0.02630440416661176, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44850 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract  \\\n",
       "0      adaboost algorithm based haar-like features ac...   \n",
       "1      in paper kind novel jigsaw ebg structure desig...   \n",
       "2      this paper studies problem autonomous underwat...   \n",
       "3      this paper presents methodology analyze lingui...   \n",
       "4      boneh durfee eurocrypt 1999 proposed polynomia...   \n",
       "...                                                  ...   \n",
       "44845  we analyse problem aggregating judgments multi...   \n",
       "44846  the utilization icts creating new jobs elimina...   \n",
       "44847  in infinite horizon inventory sales model sell...   \n",
       "44848  infrared imaging technology study deep-space b...   \n",
       "44849  taking net power output optimization objective...   \n",
       "\n",
       "                                                 authors  \\\n",
       "0      [Zheng Xu, Runbin Shi, Zhihao Sun, Yaqi Li, Yu...   \n",
       "1        [Yufei Liang, Yan Zhang, Tao Dong, Shan-wei Lu]   \n",
       "2                   [Xiaodong Ai, Keyou You, Shiji Song]   \n",
       "3       [Vincent Buntinx, Cyril Bornet, Frédéric Kaplan]   \n",
       "4                    [Atsushi Takayasu, Noboru Kunihiro]   \n",
       "...                                                  ...   \n",
       "44845         [Irem Bozbay, Franz Dietrich, Hans Peters]   \n",
       "44846  [Marios Pappas, Yannis Papagerasimou, Athanasi...   \n",
       "44847  [Anita van den Berg, Jean-Jacques Herings, Han...   \n",
       "44848  [Michele Dei, Stepan Sutula, Jose Cisneros, Er...   \n",
       "44849  [Chao He, You-zhou Jiao, Chaochao Tian, Zhenfe...   \n",
       "\n",
       "                                         id  n_citation  \\\n",
       "0      001eef4f-1d00-4ae6-8b4f-7e66344bbc6e           0   \n",
       "1      002e0b7e-d62f-4140-b015-1fe29a9acbaa           0   \n",
       "2      00352759-f0a7-4678-82ae-fed68c700da6           0   \n",
       "3      01522369-3b88-4256-99d4-4e0c1de9f1ba           0   \n",
       "4      01537b60-9ae2-4684-a1fa-e688e7757e6f           0   \n",
       "...                                     ...         ...   \n",
       "44845  fab4bc37-adaf-46a8-928f-cb232d14a574           0   \n",
       "44846  fb60d6a0-ac9f-40cf-a4b7-9dfbcd0951e7           0   \n",
       "44847  fc0809cd-e8b5-4f50-a929-e09312c59fe0           0   \n",
       "44848  fc27af72-9f8c-4bda-8f19-11100096ae59           0   \n",
       "44849  fcdeea27-105a-45bb-b34d-0eecf6f415bf           0   \n",
       "\n",
       "                                              references  \\\n",
       "0      [0a11984c-ab6e-4b75-9291-e1b700c98d52, 1f4152a...   \n",
       "1                                                     []   \n",
       "2      [1862a08a-08c6-4ab1-a214-8932bbd0d2d9, 7bcea2f...   \n",
       "3      [426b57a8-2e7d-498d-9a57-c09983ae0699, 6499d57...   \n",
       "4                                                   None   \n",
       "...                                                  ...   \n",
       "44845                                               None   \n",
       "44846                                               None   \n",
       "44847  [35a8f1b8-4e51-4928-9603-1f6a1872d02b, 460f12f...   \n",
       "44848                                               None   \n",
       "44849                                               None   \n",
       "\n",
       "                                                   title  \\\n",
       "0      a heterogeneous system real-time detection ada...   \n",
       "1          a novel conformal jigsaw ebg structure design   \n",
       "2      a source-seeking strategy autonomous underwate...   \n",
       "3      studying linguistic changes 200 years newspape...   \n",
       "4      small secret exponent attacks rsa unbalanced p...   \n",
       "...                                                  ...   \n",
       "44845                  judgment aggregation search truth   \n",
       "44846           ict-based innovation employability women   \n",
       "44847  the economic order decision continuous dynamic...   \n",
       "44848  a robust 966-db-sndr 50-khz-bandwidth switched...   \n",
       "44849  the exergy loss distribution heat transfer cap...   \n",
       "\n",
       "                                                   venue  year language  \\\n",
       "0          high performance computing and communications  2016       en   \n",
       "1      international conference on conceptual structures  2016       en   \n",
       "2      international conference on control, automatio...  2016       en   \n",
       "3                        Frontiers in Digital Humanities  2017       en   \n",
       "4      international symposium on information theory ...  2016       en   \n",
       "...                                                  ...   ...      ...   \n",
       "44845                                                     2014       en   \n",
       "44846                                                     2017       en   \n",
       "44847                        Operations Research Letters  2017       en   \n",
       "44848                                            Sensors  2017       en   \n",
       "44849                                            Entropy  2017       en   \n",
       "\n",
       "                                          abstract_words  \\\n",
       "0      [adaboost, algorithm, based, haar-like, featur...   \n",
       "1      [in, paper, kind, novel, jigsaw, ebg, structur...   \n",
       "2      [this, paper, studies, problem, autonomous, un...   \n",
       "3      [this, paper, presents, methodology, analyze, ...   \n",
       "4      [boneh, durfee, eurocrypt, 1999, proposed, pol...   \n",
       "...                                                  ...   \n",
       "44845  [we, analyse, problem, aggregating, judgments,...   \n",
       "44846  [the, utilization, icts, creating, new, jobs, ...   \n",
       "44847  [in, infinite, horizon, inventory, sales, mode...   \n",
       "44848  [infrared, imaging, technology, study, deep-sp...   \n",
       "44849  [taking, net, power, output, optimization, obj...   \n",
       "\n",
       "                                             title_words  \\\n",
       "0      [a, heterogeneous, system, real-time, detectio...   \n",
       "1      [a, novel, conformal, jigsaw, ebg, structure, ...   \n",
       "2      [a, source-seeking, strategy, autonomous, unde...   \n",
       "3      [studying, linguistic, changes, 200, years, ne...   \n",
       "4      [small, secret, exponent, attacks, rsa, unbala...   \n",
       "...                                                  ...   \n",
       "44845             [judgment, aggregation, search, truth]   \n",
       "44846      [ict-based, innovation, employability, women]   \n",
       "44847  [the, economic, order, decision, continuous, d...   \n",
       "44848  [a, robust, 966-db-sndr, 50-khz-bandwidth, swi...   \n",
       "44849  [the, exergy, loss, distribution, heat, transf...   \n",
       "\n",
       "                                   abstract_word_vectors  \\\n",
       "0      [0.11266263969710294, 0.14881521901033357, 0.0...   \n",
       "1      [0.26782341763426204, 0.08140046438692432, 0.0...   \n",
       "2      [0.15594720700207879, 0.2469724685748053, 0.06...   \n",
       "3      [-0.037486696774049574, 0.17322031537848478, 0...   \n",
       "4      [0.18371008007281173, 0.13022235570673457, -0....   \n",
       "...                                                  ...   \n",
       "44845  [0.020626525186220712, 0.05914261931967404, 0....   \n",
       "44846  [-0.1855058832864823, -0.0411357720350397, 0.4...   \n",
       "44847  [0.3582092604690021, -0.023328542408923948, 0....   \n",
       "44848  [0.21958948684480498, -0.06015576256782208, 0....   \n",
       "44849  [0.28669231532641304, 0.00015438871072014657, ...   \n",
       "\n",
       "                                      title_word_vectors  \n",
       "0      [0.1047421395778656, -0.19825784070417285, -0....  \n",
       "1      [-0.01776855013200215, 0.11805542784609964, -0...  \n",
       "2      [0.1360605876478884, -0.14277052051491207, -0....  \n",
       "3      [0.04104703077529039, 0.03502231393940747, -0....  \n",
       "4      [0.10223487787880003, -0.09928686544299126, -0...  \n",
       "...                                                  ...  \n",
       "44845  [0.07361198449507356, 0.067861866671592, -0.19...  \n",
       "44846  [-0.05307137384079397, 0.08278964925557375, 0....  \n",
       "44847  [0.19857570487591955, 0.10304184188134968, -0....  \n",
       "44848  [-0.004775028083134782, -0.016148306598717518,...  \n",
       "44849  [0.09038282524455678, -0.02630440416661176, -0...  \n",
       "\n",
       "[44850 rows x 13 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#papers_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method for number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"concat(abstract_word_vectors, title_word_vectors)\" due to data type mismatch: Parameter 1 requires the (\"STRING\" or \"BINARY\" or \"ARRAY\") type, however \"abstract_word_vectors\" has the type \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\".;\n'Project [concat(abstract_word_vectors#495, title_word_vectors#521) AS features#598]\n+- Project [abstract_word_vectors#495, title_word_vectors#521]\n   +- Project [abstract#379, authors#9, id#10, n_citation#11L, references#12, title#390, venue#14, year#15L, language#304, abstract_words#454, title_words#471, abstract_word_vectors#495, UDF(title_words#471) AS title_word_vectors#521]\n      +- Project [abstract#379, authors#9, id#10, n_citation#11L, references#12, title#390, venue#14, year#15L, language#304, abstract_words#454, title_words#471, UDF(abstract_words#454) AS abstract_word_vectors#495]\n         +- Project [abstract#379, authors#9, id#10, n_citation#11L, references#12, title#390, venue#14, year#15L, language#304, abstract_words#454, UDF(title#390) AS title_words#471]\n            +- Project [abstract#379, authors#9, id#10, n_citation#11L, references#12, title#390, venue#14, year#15L, language#304, UDF(abstract#379) AS abstract_words#454]\n               +- Project [abstract#379, authors#9, id#10, n_citation#11L, references#12, remove_punctuation(title#368)#389 AS title#390, venue#14, year#15L, language#304]\n                  +- Project [remove_punctuation(abstract#357)#378 AS abstract#379, authors#9, id#10, n_citation#11L, references#12, title#368, venue#14, year#15L, language#304]\n                     +- Project [abstract#357, authors#9, id#10, n_citation#11L, references#12, remove_custom_stop_words(title#346)#367 AS title#368, venue#14, year#15L, language#304]\n                        +- Project [remove_custom_stop_words(abstract#336)#356 AS abstract#357, authors#9, id#10, n_citation#11L, references#12, title#346, venue#14, year#15L, language#304]\n                           +- Project [abstract#336, authors#9, id#10, n_citation#11L, references#12, lower(title#326) AS title#346, venue#14, year#15L, language#304]\n                              +- Project [lower(abstract#315) AS abstract#336, authors#9, id#10, n_citation#11L, references#12, title#326, venue#14, year#15L, language#304]\n                                 +- Project [abstract#315, authors#9, id#10, n_citation#11L, references#12, remove_stop_words(title#13)#325 AS title#326, venue#14, year#15L, language#304]\n                                    +- Project [remove_stop_words(abstract#8)#314 AS abstract#315, authors#9, id#10, n_citation#11L, references#12, title#13, venue#14, year#15L, language#304]\n                                       +- Filter (language#304 = en)\n                                          +- Project [abstract#8, authors#9, id#10, n_citation#11L, references#12, title#13, venue#14, year#15L, detect_language(abstract#8)#303 AS language#304]\n                                             +- Relation [abstract#8,authors#9,id#10,n_citation#11L,references#12,title#13,venue#14,year#15L] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m cost \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(n)\n\u001b[1;32m     10\u001b[0m papers_df_vectors \u001b[38;5;241m=\u001b[39m papers_df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabstract_word_vectors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle_word_vectors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mpapers_df_vectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpapers_df_vectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabstract_word_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpapers_df_vectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtitle_word_vectors\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m,n):\n\u001b[1;32m     14\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m KMeans(k\u001b[38;5;241m=\u001b[39mk, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3223\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m \n\u001b[1;32m   3181\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3223\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"concat(abstract_word_vectors, title_word_vectors)\" due to data type mismatch: Parameter 1 requires the (\"STRING\" or \"BINARY\" or \"ARRAY\") type, however \"abstract_word_vectors\" has the type \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\".;\n'Project [concat(abstract_word_vectors#495, title_word_vectors#521) AS features#598]\n+- Project [abstract_word_vectors#495, title_word_vectors#521]\n   +- Project [abstract#379, authors#9, id#10, n_citation#11L, references#12, title#390, venue#14, year#15L, language#304, abstract_words#454, title_words#471, abstract_word_vectors#495, UDF(title_words#471) AS title_word_vectors#521]\n      +- Project [abstract#379, authors#9, id#10, n_citation#11L, references#12, title#390, venue#14, year#15L, language#304, abstract_words#454, title_words#471, UDF(abstract_words#454) AS abstract_word_vectors#495]\n         +- Project [abstract#379, authors#9, id#10, n_citation#11L, references#12, title#390, venue#14, year#15L, language#304, abstract_words#454, UDF(title#390) AS title_words#471]\n            +- Project [abstract#379, authors#9, id#10, n_citation#11L, references#12, title#390, venue#14, year#15L, language#304, UDF(abstract#379) AS abstract_words#454]\n               +- Project [abstract#379, authors#9, id#10, n_citation#11L, references#12, remove_punctuation(title#368)#389 AS title#390, venue#14, year#15L, language#304]\n                  +- Project [remove_punctuation(abstract#357)#378 AS abstract#379, authors#9, id#10, n_citation#11L, references#12, title#368, venue#14, year#15L, language#304]\n                     +- Project [abstract#357, authors#9, id#10, n_citation#11L, references#12, remove_custom_stop_words(title#346)#367 AS title#368, venue#14, year#15L, language#304]\n                        +- Project [remove_custom_stop_words(abstract#336)#356 AS abstract#357, authors#9, id#10, n_citation#11L, references#12, title#346, venue#14, year#15L, language#304]\n                           +- Project [abstract#336, authors#9, id#10, n_citation#11L, references#12, lower(title#326) AS title#346, venue#14, year#15L, language#304]\n                              +- Project [lower(abstract#315) AS abstract#336, authors#9, id#10, n_citation#11L, references#12, title#326, venue#14, year#15L, language#304]\n                                 +- Project [abstract#315, authors#9, id#10, n_citation#11L, references#12, remove_stop_words(title#13)#325 AS title#326, venue#14, year#15L, language#304]\n                                    +- Project [remove_stop_words(abstract#8)#314 AS abstract#315, authors#9, id#10, n_citation#11L, references#12, title#13, venue#14, year#15L, language#304]\n                                       +- Filter (language#304 = en)\n                                          +- Project [abstract#8, authors#9, id#10, n_citation#11L, references#12, title#13, venue#14, year#15L, detect_language(abstract#8)#303 AS language#304]\n                                             +- Relation [abstract#8,authors#9,id#10,n_citation#11L,references#12,title#13,venue#14,year#15L] json\n"
     ]
    }
   ],
   "source": [
    "# Calculate cost and plot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.sql.functions import concat\n",
    "\n",
    "n=15\n",
    "cost = np.zeros(n)\n",
    "\n",
    "\n",
    "#papers_df_vectors = papers_df.select(\"abstract_word_vectors\", \"title_word_vectors\")\n",
    "#papers_df_vectors.select(concat(papers_df_vectors.abstract_word_vectors, papers_df_vectors.title_word_vectors).alias(\"features\")).collect()\n",
    "\n",
    "# Dataset has to have one column with the name 'features' that the model uses. \n",
    "# But what info and in which form should it be?\n",
    "\n",
    "\n",
    "for k in range(2,n):\n",
    "    kmeans = KMeans(k=k, seed=3)\n",
    "    model = kmeans.fit(papers_df_vectors)\n",
    "    #cost[k] = model.computeCost(papers_df_vectors)\n",
    "    clusterdData = model.transform(papers_df_vectors)\n",
    "    evaluator = ClusteringEvaluator()\n",
    "    cost[k] = evaluator.evaluate(clusterdData)\n",
    "\n",
    "# Plot the cost\n",
    "df_cost = pd.DataFrame(cost[2:])\n",
    "df_cost.columns = [\"cost\"]\n",
    "new_col = range(2,n)\n",
    "df_cost.insert(0, 'cluster', new_col)\n",
    "\n",
    "import pylab as pl\n",
    "pl.plot(df_cost.cluster, df_cost.cost)\n",
    "pl.xlabel('Number of Clusters')\n",
    "pl.ylabel('Score')\n",
    "pl.title('Elbow Curve')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means (from practise session)\n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=3, seed=42)\n",
    "model = kmeans.fit(transformed_data) # What shape should transformed_data be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the clusters using the model\n",
    "clusterdData = model.transform(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating: the silhouette value\n",
    "\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(clusterdData)\n",
    "print('Silhouette with squared euclidean distance = ', silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommender function\n",
    "\n",
    "# Some ideas:\n",
    "# https://asdkazmi.medium.com/ai-movies-recommendation-system-with-clustering-based-k-means-algorithm-f04467e02fcd\n",
    "# https://www.activestate.com/blog/exploring-k-means-clustering-in-big-data-using-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
