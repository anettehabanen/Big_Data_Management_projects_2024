{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6565e3f8",
   "metadata": {},
   "source": [
    "## Project Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55974225",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .config(\"spark.jars.packages\", 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0') \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc455f-639e-4ebf-8b55-90199a17beff",
   "metadata": {},
   "source": [
    "### Starting the Kafka stream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35005b8",
   "metadata": {},
   "source": [
    "Be sure to start the stream on Kafka!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "163f7900-765e-4c70-8bb3-e2ca314ab0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import SerializingProducer, DeserializingConsumer\n",
    "from confluent_kafka.serialization import StringSerializer, StringDeserializer\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "from uuid import uuid4\n",
    "import sys, random\n",
    "\n",
    "brokers = \"kafka1:9092,kafka2:9093\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35ae0b96-c2a5-4593-aa95-a80bf24cdfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pconf = {\n",
    "    'bootstrap.servers': brokers,\n",
    "    'partitioner': 'murmur2_random',\n",
    "    'key.serializer': StringSerializer('utf_8'),\n",
    "    'value.serializer':  StringSerializer('utf_8')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c09fad36-ee3e-42dd-b2fb-9ac44d2109c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = SerializingProducer(pconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c544f77d-37bd-4ffb-9722-1b517131d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxies = \"sample.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8330f06d-7598-417b-a663-10c6f0871513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime  \n",
    "  #2023-10-13T08:16:13Z\n",
    "def construct_ride(row):\n",
    "    time_stamp = time.time()\n",
    "    date_time = datetime.fromtimestamp(time_stamp)\n",
    "    str_date_time = date_time.strftime(\"%Y-%m-%dT%H:%M:%SZ\") #\"%d-%m-%Y, %H:%M:%S\"\n",
    "    ride = {\"vendor_id\": row[2],\n",
    "            \"pickup_datetime\": row[5],\n",
    "            \"droppff_datetime\":row[5],\n",
    "            \"pickup_longitude\": float(row[10]),\n",
    "            \"pickup_latitude\": float(row[11]),\n",
    "            \"dropoff_longitude\": float(row[12]),\n",
    "            \"dropoff_latitude\": float(row[13]),\n",
    "             \"timestamp\":str_date_time\n",
    "             }\n",
    "    return ride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf421eef-4076-42fc-a3ac-dccd43810331",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m         p\u001b[38;5;241m.\u001b[39mpoll(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;66;03m#p.flush()\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m#n = n + 1\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBufferError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv, json\n",
    "import time\n",
    "n = 0\n",
    "with open(taxies) as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    next(csv_reader, None) # Skipping the headers\n",
    "    try:\n",
    "        for row in csv_reader:\n",
    "            #if n == 10:\n",
    "             #  break\n",
    "            ride = construct_ride(row)\n",
    "            #print(ride)\n",
    "            p.produce('ride', value=json.dumps(ride))\n",
    "            p.poll(0)\n",
    "            #p.flush()\n",
    "            time.sleep(0.5)\n",
    "            #n = n + 1\n",
    "    except BufferError:\n",
    "        sys.stderr.write('%% Local producer queue is full (%d messages awaiting delivery): try again\\n' % len(p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd4ca52-a930-445f-ac1c-db2a629cd5bb",
   "metadata": {},
   "source": [
    "### Reading the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e622d5b-ef4d-4d78-abcb-e9edc2305ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, BooleanType, TimestampType, DateType\n",
    "# TO MODIFY FOR YOUR SCHEMA\n",
    "schema = StructType(\n",
    "      [\n",
    "        StructField(\"vendor_id\", StringType(), False),\n",
    "        StructField(\"pickup_datetime\", StringType(), False),\n",
    "        StructField(\"droppff_datetime\", StringType(), False),\n",
    "        StructField(\"pickup_longitude\", DoubleType(), False),\n",
    "        StructField(\"pickup_latitude\", DoubleType(), False),\n",
    "        StructField(\"dropoff_longitude\", DoubleType(), False),Ãµ\n",
    "        StructField(\"dropoff_latitude\", DoubleType(), False),\n",
    "        StructField(\"timestamp\", TimestampType(), False),\n",
    "      ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69712d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_server = \"kafka1:9092\"   \n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "lines = (spark.readStream                        # Get the DataStreamReader\n",
    "  .format(\"kafka\")                                 # Specify the source format as \"kafka\"\n",
    "  .option(\"kafka.bootstrap.servers\", kafka_server) # Configure the Kafka server name and port\n",
    "  .option(\"subscribe\", \"stock\")                       # Subscribe to the \"en\" Kafka topic \n",
    "  .option(\"startingOffsets\", \"earliest\")           # The start point when a query is started\n",
    "  .option(\"maxOffsetsPerTrigger\", 100)             # Rate limit on max offsets per trigger interval\n",
    "  .load()\n",
    "# Load the DataFrame\n",
    ")\n",
    "df = lines.select(\"parsed_value.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08919906-1dc8-4c82-a4f7-e1c6f6ea9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a46c68-44ab-4e3a-90fb-d334423e4acc",
   "metadata": {},
   "source": [
    "## The project starts here\n",
    "\n",
    "You can create a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24187ef-e5b4-4fa7-bab5-60aa94412a05",
   "metadata": {},
   "source": [
    "## [Query 1] Utilization over a window of 5, 10, and 15 minutes per taxi/driver. This can be computed by computing the idle time per taxi. How does it change? Is there an optimal window?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b2d7ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6746caef-fc7c-4d0e-98df-cdd6046393eb",
   "metadata": {},
   "source": [
    "## [Query 2] The average time it takes for a taxi to find its next fare(trip) per destination borough. This can be computed by finding the time difference, e.g. in seconds, between the trip's drop off and the next trip's pick up within a given unit of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bea404b-fc76-48f9-83d9-5946617863de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember you can register another stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268c285-d55c-4be3-8e5a-e7ddebb14153",
   "metadata": {},
   "source": [
    "## [Query 3] The number of trips that started and ended within the same borough in the last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7ad21-59c2-4d4c-befe-9d1ceedbb74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember you can register another stream\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d0d685-c3ed-4b7d-8ebc-c3174ba55645",
   "metadata": {},
   "source": [
    "## [Query 4] The number of trips that started in one borough and ended in another one in the last hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3578b-1960-4969-801b-adc2f45493a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
